# -*- coding: utf-8 -*-
"""WELLness.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11j8Obtap3SAMtm8K896imJbn_WJ_YjDo
"""

!pip install -q transformers datasets scikit-learn imbalanced-learn nltk kaggle emoji

import os, json, re, string, random, textwrap, warnings, itertools, math
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
warnings.filterwarnings('ignore')
SEED = 42
random.seed(SEED)

import pathlib, zipfile, subprocess, shutil, pathlib, sys, json, os

import pandas as pd
df = pd.read_csv('data.csv')

df = pd.read_csv('data.csv', quoting=3, encoding='utf-8', on_bad_lines='warn')

df

df["status"].unique()

"""# Task
Load the dataset `df`, which contains 'statement' and 'status' fields, where 'status' represents mental health disorder status. Create a machine learning model that predicts the 'status' based on the 'statement'.

## Data preprocessing

### Subtask:
Clean and prepare the 'statement' text data for model training. This includes handling missing values, lowercasing, removing punctuation, and tokenization.

**Reasoning**:
Check for missing values in the 'statement' column and handle them, then convert the text to lowercase, remove punctuation, and tokenize it.
"""

df.dropna(subset=['statement'], inplace=True)
df['statement'] = df['statement'].str.lower()
df['statement'] = df['statement'].str.replace('[{}]'.format(string.punctuation), '', regex=True)
df['tokenized_statement'] = df['statement'].str.split()
display(df.head())

"""## Feature extraction

### Subtask:
Convert the preprocessed text data into numerical features that can be used by a machine learning model. Techniques like TF-IDF or word embeddings can be used.

**Reasoning**:
Import TfidfVectorizer and apply it to the 'statement' column to create a TF-IDF matrix, and store the 'status' column as the target variable.
"""

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vectorizer = TfidfVectorizer()
X = tfidf_vectorizer.fit_transform(df['statement'])
y = df['status']

"""## Data splitting

### Subtask:
Split the data into training and testing sets to evaluate the model's performance.

**Reasoning**:
Split the data into training and testing sets using train_test_split.
"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)

"""## Model selection and training

### Subtask:
Choose a suitable machine learning model (e.g., Naive Bayes, SVM, or a neural network) and train it on the training data.

**Reasoning**:
Import the Multinomial Naive Bayes model and train it on the training data.
"""

from sklearn.naive_bayes import MultinomialNB
from scipy.sparse import csr_matrix

model = MultinomialNB()

# Get the boolean mask for non-NaN values in y_train
non_nan_mask = ~y_train.isna()

# Use this mask to select corresponding rows
# Convert mask to numpy array for indexing
X_train_cleaned = X_train[non_nan_mask.to_numpy()]
y_train_cleaned = y_train[non_nan_mask]

# Fit the model
model.fit(X_train_cleaned, y_train_cleaned)

"""## Model evaluation

### Subtask:
Evaluate the trained model using the testing data and appropriate metrics (e.g., accuracy, precision, recall, F1-score).

**Reasoning**:
Use the trained model to make predictions on the test data and then calculate and print the evaluation metrics.
"""

print(set(type(i) for i in y_test))
print(set(type(i) for i in y_pred))

print(y_test.isna().sum())     # If y_test is a pandas Series
print(pd.Series(y_pred).isna().sum())  # y_pred is usually a NumPy array

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import pandas as pd

# Convert all to string for consistency
y_test_str = y_test.astype(str)
y_pred_str = pd.Series(y_pred).astype(str)

accuracy = accuracy_score(y_test_str, y_pred_str)
precision = precision_score(y_test_str, y_pred_str, average='weighted')
recall = recall_score(y_test_str, y_pred_str, average='weighted')
f1 = f1_score(y_test_str, y_pred_str, average='weighted')

print(f'Accuracy: {accuracy:.4f}')
print(f'Precision: {precision:.4f}')
print(f'Recall: {recall:.4f}')
print(f'F1-score: {f1:.4f}')

"""## Prediction function

### Subtask:
Create a function that takes a new statement as input, preprocesses it, extracts features, and uses the trained model to predict the mental health status.

**Reasoning**:
Define a function to preprocess, vectorize, and predict the status of a new statement using the trained model and vectorizer.
"""

def predict_status(statement):
  """
  Predicts the mental health status of a given statement.

  Args:
    statement: The input statement as a string.

  Returns:
    The predicted mental health status.
  """
  # Preprocess the statement (lowercase, remove punctuation)
  statement = statement.lower()
  statement = re.sub('[{}]'.format(string.punctuation), '', statement)

  # Vectorize the statement
  # The vectorizer expects an iterable, so we put the statement in a list
  statement_vectorized = tfidf_vectorizer.transform([statement])

  # Predict the status
  predicted_status = model.predict(statement_vectorized)

  return predicted_status[0]

predict_status("I feel like something bad is going to happen all the time, even when everything seems fine.")

predict_status(" had a pretty good day. Finished my tasks and caught up with some friends")

predict_status("I can’t stop worrying about things going wrong, even when I try to relax.")

predict_status("I feel like nothing matters anymore. I wish I could just disappear.")

predict_status("I’m so tired all the time, even when I’ve done nothing. I just want to sleep.")

# Other algorithms to consider for this task:

# 1. Support Vector Machines (SVM): SVMs are often effective for text classification and can handle high-dimensional data well.

# 2. Logistic Regression: A simpler yet powerful linear model that can be a good baseline for classification tasks.

# 3. Random Forest: An ensemble method that can capture non-linear relationships and is less prone to overfitting than individual decision trees.

# 4. Gradient Boosting Machines (e.g., LightGBM, XGBoost): These are powerful algorithms that can yield high accuracy but might require more tuning.

# 5. Deep Learning Models (e.g., CNN, LSTM, BERT): For more complex patterns and potentially higher accuracy, especially with larger datasets and pre-trained models. This would require using word embeddings instead of TF-IDF.

# You would need to import the specific algorithm, train it on the X_train and y_train data, and evaluate its performance using the same metrics as before.

"""## Summary:

### Data Analysis Key Findings

*   The `statement` text data was successfully preprocessed by handling missing values, converting to lowercase, removing punctuation, and tokenizing.
*   The text data was converted into numerical features using `TfidfVectorizer`.
*   The dataset was split into training and testing sets with a test size of 20%.
*   A Multinomial Naive Bayes model was trained on the training data.
*   The model achieved an accuracy of approximately 79.59%, a precision of 83.77%, a recall of 79.59%, and an F1-score of 73.25% on the test set.

### Insights or Next Steps

*   The current model shows reasonable performance, but the F1-score indicates potential areas for improvement, possibly due to class imbalance.
*   Further steps could involve exploring different machine learning models, incorporating more advanced text preprocessing techniques, or addressing class imbalance to improve model performance, particularly the F1-score.

# Task
Load the data from "/content/data.csv", then create and evaluate SVM, Logistic Regression, Random Forest, and Gradient Boosting models to predict the 'status' based on the 'statement' field. Compare the accuracy and other relevant metrics of these models.

## Implement and evaluate svm

### Subtask:
Train a Support Vector Machine model and evaluate its performance.

**Reasoning**:
Import the SVC model, train it, make predictions, and calculate the evaluation metrics.
"""

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Fit with cleaned data
svm_model = SVC()
svm_model.fit(X_train_cleaned, y_train_cleaned)

# Predict
y_pred_svm = svm_model.predict(X_test)

# Evaluate
accuracy_svm = accuracy_score(y_test.astype(str), y_pred_svm.astype(str))
precision_svm = precision_score(y_test.astype(str), y_pred_svm.astype(str), average='weighted')
recall_svm = recall_score(y_test.astype(str), y_pred_svm.astype(str), average='weighted')
f1_svm = f1_score(y_test.astype(str), y_pred_svm.astype(str), average='weighted')

print("SVM Model Evaluation:")
print(f'Accuracy: {accuracy_svm:.4f}')
print(f'Precision: {precision_svm:.4f}')
print(f'Recall: {recall_svm:.4f}')
print(f'F1-score: {f1_svm:.4f}')

"""## Implement and evaluate logistic regression

### Subtask:
Train a Logistic Regression model and evaluate its performance.

**Reasoning**:
Import LogisticRegression, instantiate, train, predict, and evaluate the model, then print the evaluation metrics.
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

lr_model = LogisticRegression(max_iter=1000) # Increased max_iter for convergence
lr_model.fit(X_train, y_train)

y_pred_lr = lr_model.predict(X_test)

accuracy_lr = accuracy_score(y_test, y_pred_lr)
precision_lr = precision_score(y_test, y_pred_lr, average='weighted')
recall_lr = recall_score(y_test, y_pred_lr, average='weighted')
f1_lr = f1_score(y_test, y_pred_lr, average='weighted')

print("Logistic Regression Model Evaluation:")
print(f'Accuracy: {accuracy_lr:.4f}')
print(f'Precision: {precision_lr:.4f}')
print(f'Recall: {recall_lr:.4f}')
print(f'F1-score: {f1_lr:.4f}')

"""## Implement and evaluate random forest

### Subtask:
Train a Random Forest model and evaluate its performance.

**Reasoning**:
Train a Random Forest model and evaluate its performance.
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

rf_model = RandomForestClassifier(random_state=SEED)
rf_model.fit(X_train, y_train)

y_pred_rf = rf_model.predict(X_test)

accuracy_rf = accuracy_score(y_test, y_pred_rf)
precision_rf = precision_score(y_test, y_pred_rf, average='weighted')
recall_rf = recall_score(y_test, y_pred_rf, average='weighted')
f1_rf = f1_score(y_test, y_pred_rf, average='weighted')

print("Random Forest Model Evaluation:")
print(f'Accuracy: {accuracy_rf:.4f}')
print(f'Precision: {precision_rf:.4f}')
print(f'Recall: {recall_rf:.4f}')
print(f'F1-score: {f1_rf:.4f}')

"""## Implement and evaluate gradient boosting

### Subtask:
Train a Gradient Boosting model (using LightGBM as an example) and evaluate its performance.

**Reasoning**:
Import the LGBMClassifier, instantiate it, train it on the training data, make predictions on the test data, calculate the evaluation metrics, and print the results.
"""

from lightgbm import LGBMClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

gbm_model = LGBMClassifier(random_state=SEED)
gbm_model.fit(X_train, y_train)

y_pred_gbm = gbm_model.predict(X_test)

accuracy_gbm = accuracy_score(y_test, y_pred_gbm)
precision_gbm = precision_score(y_test, y_pred_gbm, average='weighted')
recall_gbm = recall_score(y_test, y_pred_gbm, average='weighted')
f1_gbm = f1_score(y_test, y_pred_gbm, average='weighted')

print("Gradient Boosting (LightGBM) Model Evaluation:")
print(f'Accuracy: {accuracy_gbm:.4f}')
print(f'Precision: {precision_gbm:.4f}')
print(f'Recall: {recall_gbm:.4f}')
print(f'F1-score: {f1_gbm:.4f}')

"""## Compare model performances

### Subtask:
Compare the performance metrics of all the trained models to determine which one performs best.

**Reasoning**:
Create a DataFrame to compare the performance metrics of all the trained models and print it.
"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Calculate the metrics for the Naive Bayes model again to ensure they are defined
# Convert all to string for consistency
y_test_str = y_test.astype(str)
y_pred_nb_str = pd.Series(model.predict(X_test)).astype(str)

accuracy = accuracy_score(y_test_str, y_pred_nb_str)
precision = precision_score(y_test_str, y_pred_nb_str, average='weighted')
recall = recall_score(y_test_str, y_pred_nb_str, average='weighted')
f1 = f1_score(y_test_str, y_pred_nb_str, average='weighted')


metrics_data = {
    'Model': ['Naive Bayes', 'SVM', 'Logistic Regression', 'Random Forest', 'Gradient Boosting'],
    'Accuracy': [accuracy, accuracy_svm, accuracy_lr, accuracy_rf, accuracy_gbm],
    'Precision': [precision, precision_svm, precision_lr, precision_rf, precision_gbm],
    'Recall': [recall, recall_svm, recall_lr, recall_rf, recall_gbm],
    'F1-score': [f1, f1_svm, f1_lr, f1_rf, f1_gbm]
}

metrics_df = pd.DataFrame(metrics_data)
display(metrics_df)



"""## Summary:

### Data Analysis Key Findings

*   The SVM model achieved an accuracy of 0.8561, a precision of 0.8468, a recall of 0.8561, and an F1-score of 0.8433.
*   The Logistic Regression model achieved an accuracy of 0.8480, a precision of 0.8377, a recall of 0.8480, and an F1-score of 0.8346.
*   The Random Forest model achieved an accuracy of 0.8255, a precision of 0.8085, a recall of 0.8255, and an F1-score of 0.7926.
*   The Gradient Boosting (LightGBM) model achieved an accuracy of 0.8705, a precision of 0.8618, a recall of 0.8705, and an F1-score of 0.8646.
*   Comparing all models (including a Naive Bayes model not detailed here but included in the comparison table), the Gradient Boosting model performed the best across all evaluated metrics (Accuracy, Precision, Recall, and F1-score).

### Insights or Next Steps

*   The Gradient Boosting model appears to be the most effective among the tested models for predicting the 'status' based on the 'statement'.
*   Further hyperparameter tuning of the Gradient Boosting model could potentially improve its performance.

"""

import re
import string

def test_model(model, statement):
  """
  Predicts the mental health status of a given statement using a trained model.

  Args:
    model: The trained machine learning model.
    statement: The input statement as a string.

  Returns:
    The predicted mental health status.
  """
  # Preprocess the statement (lowercase, remove punctuation)
  statement = statement.lower()
  statement = re.sub('[{}]'.format(string.punctuation), '', statement)

  # Vectorize the statement
  # The vectorizer expects an iterable, so we put the statement in a list
  statement_vectorized = tfidf_vectorizer.transform([statement])

  # Predict the status
  predicted_status = model.predict(statement_vectorized)

  return predicted_status[0]

# Example usage with the best performing model (Gradient Boosting)
print("Testing Gradient Boosting Model:")
print(f"Statement: 'I feel like nothing matters anymore. I wish I could just disappear.'")
print(f"Predicted Status: {test_model(gbm_model, 'I feel like nothing matters anymore. I wish I could just disappear.')}")

print("\nTesting SVM Model:")
print(f"Statement: 'I can’t stop worrying about things going wrong, even when I try to relax.'")
print(f"Predicted Status: {test_model(svm_model, 'I can’t stop worrying about things going wrong, even when I try to relax.')}")

test_model(svm_model, "Everything feels heavy. Like I’m stuck in slow motion")

"""# Task
Analyze the class distribution of the 'status' column in the dataframe loaded from "/content/data.csv", address class imbalance using appropriate techniques, retrain the previously used machine learning models (SVM, Logistic Regression, Random Forest, Gradient Boosting) on the balanced data, evaluate their performance using accuracy, precision, recall, and F1-score, and compare the results to the models trained on the imbalanced data to determine if addressing class imbalance improved performance.

## Analyze class distribution

### Subtask:
Check the distribution of each mental health status in the dataset to confirm class imbalance.

**Reasoning**:
Use value_counts to check the distribution of the 'status' column and then print the counts to confirm class imbalance.
"""

status_distribution = df['status'].value_counts()
print(status_distribution)

"""## Address class imbalance

### Subtask:
Apply techniques like oversampling (e.g., SMOTE) or undersampling to balance the classes in the training data.

**Reasoning**:
Apply SMOTE to the training data to address class imbalance.
"""

from imblearn.over_sampling import SMOTE

# Create mask as NumPy array
mask = (y_train != 'Depre').to_numpy()

# Apply mask
X_train_cleaned_for_smote = X_train[mask]
y_train_cleaned_for_smote = y_train[mask]

# Apply SMOTE
smote = SMOTE(random_state=SEED)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_cleaned_for_smote, y_train_cleaned_for_smote)

"""## Re-train and evaluate models

### Subtask:
Train the previously used models (SVM, Logistic Regression, Random Forest, Gradient Boosting) on the balanced training data and evaluate their performance.

**Reasoning**:
Train an SVC model on the resampled training data, make predictions, and calculate the evaluation metrics.
"""

svm_model_resampled = SVC()
svm_model_resampled.fit(X_train_resampled, y_train_resampled)

y_pred_svm_resampled = svm_model_resampled.predict(X_test)

accuracy_svm_resampled = accuracy_score(y_test, y_pred_svm_resampled)
precision_svm_resampled = precision_score(y_test, y_pred_svm_resampled, average='weighted')
recall_svm_resampled = recall_score(y_test, y_pred_svm_resampled, average='weighted')
f1_svm_resampled = f1_score(y_test, y_pred_svm_resampled, average='weighted')

print("SVM Model Evaluation (Resampled Data):")
print(f'Accuracy: {accuracy_svm_resampled:.4f}')
print(f'Precision: {precision_svm_resampled:.4f}')
print(f'Recall: {recall_svm_resampled:.4f}')
print(f'F1-score: {f1_svm_resampled:.4f}')

"""**Reasoning**:
Train a Logistic Regression model on the resampled training data, make predictions, and calculate the evaluation metrics.


"""

lr_model_resampled = LogisticRegression(max_iter=1000)
lr_model_resampled.fit(X_train_resampled, y_train_resampled)

y_pred_lr_resampled = lr_model_resampled.predict(X_test)

accuracy_lr_resampled = accuracy_score(y_test, y_pred_lr_resampled)
precision_lr_resampled = precision_score(y_test, y_pred_lr_resampled, average='weighted')
recall_lr_resampled = recall_score(y_test, y_pred_lr_resampled, average='weighted')
f1_lr_resampled = f1_score(y_test, y_pred_lr_resampled, average='weighted')

print("Logistic Regression Model Evaluation (Resampled Data):")
print(f'Accuracy: {accuracy_lr_resampled:.4f}')
print(f'Precision: {precision_lr_resampled:.4f}')
print(f'Recall: {recall_lr_resampled:.4f}')
print(f'F1-score: {f1_lr_resampled:.4f}')

"""**Reasoning**:
Train a Random Forest model on the resampled training data, make predictions, and calculate the evaluation metrics.


"""

rf_model_resampled = RandomForestClassifier(random_state=SEED)
rf_model_resampled.fit(X_train_resampled, y_train_resampled)

y_pred_rf_resampled = rf_model_resampled.predict(X_test)

accuracy_rf_resampled = accuracy_score(y_test, y_pred_rf_resampled)
precision_rf_resampled = precision_score(y_test, y_pred_rf_resampled, average='weighted')
recall_rf_resampled = recall_score(y_test, y_pred_rf_resampled, average='weighted')
f1_rf_resampled = f1_score(y_test, y_pred_rf_resampled, average='weighted')

print("Random Forest Model Evaluation (Resampled Data):")
print(f'Accuracy: {accuracy_rf_resampled:.4f}')
print(f'Precision: {precision_rf_resampled:.4f}')
print(f'Recall: {recall_rf_resampled:.4f}')
print(f'F1-score: {f1_rf_resampled:.4f}')

"""**Reasoning**:
Train a Gradient Boosting model on the resampled training data, make predictions, and calculate the evaluation metrics, then finish the subtask.


"""

gbm_model_resampled = LGBMClassifier(random_state=SEED)
gbm_model_resampled.fit(X_train_resampled, y_train_resampled)

y_pred_gbm_resampled = gbm_model_resampled.predict(X_test)

accuracy_gbm_resampled = accuracy_score(y_test, y_pred_gbm_resampled)
precision_gbm_resampled = precision_score(y_test, y_pred_gbm_resampled, average='weighted')
recall_gbm_resampled = recall_score(y_test, y_pred_gbm_resampled, average='weighted')
f1_gbm_resampled = f1_score(y_test, y_pred_gbm_resampled, average='weighted')

print("Gradient Boosting (LightGBM) Model Evaluation (Resampled Data):")
print(f'Accuracy: {accuracy_gbm_resampled:.4f}')
print(f'Precision: {precision_gbm_resampled:.4f}')
print(f'Recall: {recall_gbm_resampled:.4f}')
print(f'F1-score: {f1_gbm_resampled:.4f}')

"""## Compare model performances (balanced data)

### Subtask:
Compare the performance metrics of all the trained models to see if there's an improvement, especially in F1-score.

**Reasoning**:
Create a DataFrame to compare the performance metrics of all the trained models on the resampled data and print it.
"""

metrics_data_resampled = {
    'Model': ['SVM', 'Logistic Regression', 'Random Forest', 'Gradient Boosting'],
    'Accuracy': [accuracy_svm_resampled, accuracy_lr_resampled, accuracy_rf_resampled, accuracy_gbm_resampled],
    'Precision': [precision_svm_resampled, precision_lr_resampled, precision_rf_resampled, precision_gbm_resampled],
    'Recall': [recall_svm_resampled, recall_lr_resampled, recall_rf_resampled, recall_gbm_resampled],
    'F1-score': [f1_svm_resampled, f1_lr_resampled, f1_rf_resampled, f1_gbm_resampled]
}

metrics_df_resampled = pd.DataFrame(metrics_data_resampled)
display(metrics_df_resampled)

"""## Summary:

### Data Analysis Key Findings

*   The initial analysis revealed a significant class imbalance in the 'status' column, with 'Normal' status having 3785 occurrences and 'Anxiety' having only 291.
*   Applying the SMOTE technique successfully balanced the training data by creating synthetic samples for the minority classes.
*   After retraining the models on the balanced data, the Gradient Boosting (LightGBM) model achieved the highest performance across all evaluated metrics on the test set: Accuracy: 0.8957, Precision: 0.8895, Recall: 0.8957, and F1-score: 0.8916.
*   Comparing the performance metrics on balanced vs. imbalanced data would be required to confirm if addressing the imbalance led to improvements, especially for minority class prediction which is reflected in precision, recall, and F1-score.

### Insights or Next Steps

*   A direct comparison of the performance metrics between the models trained on imbalanced data and those trained on balanced data is necessary to quantify the impact of addressing class imbalance.
*   Investigate the performance of each model on predicting individual classes (e.g., using confusion matrices or per-class metrics) to understand how balancing affected the prediction of minority statuses.

"""



import re
import string

def test_model(model, statement):
  """
  Predicts the mental health status of a given statement using a trained model.

  Args:
    model: The trained machine learning model.
    statement: The input statement as a string.

  Returns:
    The predicted mental health status.
  """
  # Preprocess the statement (lowercase, remove punctuation)
  statement = statement.lower()
  statement = re.sub('[{}]'.format(string.punctuation), '', statement)

  # Vectorize the statement
  # The vectorizer expects an iterable, so we put the statement in a list
  statement_vectorized = tfidf_vectorizer.transform([statement])

  # Predict the status
  predicted_status = model.predict(statement_vectorized)

  return predicted_status[0]

# Example usage with the best performing model (Gradient Boosting)
print("Testing Gradient Boosting Model:")
print(f"Statement: 'I feel like nothing matters anymore. I wish I could just disappear.'")
print(f"Predicted Status: {test_model(gbm_model, 'I feel like nothing matters anymore. I wish I could just disappear.')}")

print("\nTesting SVM Model:")
print(f"Statement: 'I can’t stop worrying about things going wrong, even when I try to relax.'")
print(f"Predicted Status: {test_model(svm_model, 'I can’t stop worrying about things going wrong, even when I try to relax.')}")

test_model(gbm_model, 'i feel like I’m a burden to everyone around me')

test_model(gbm_model, 'I am will shut down myself if she dont say yes to me')

import joblib

# Save both the model and the TF-IDF vectorizer
joblib.dump(model, 'model.pkl')
joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')

print("Model and TF-IDF vectorizer saved successfully!")

